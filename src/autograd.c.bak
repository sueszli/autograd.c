#include "ops/arithmetic.h"
#include "autograd.h"
#include "tensor.h"
#include <assert.h>
#include <math.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define MAX_RECURSION_DEPTH 1024
#define MAX_NDIM 32
#define MAX_ARRAY_SIZE 1000000
#define MAX_TENSOR_SIZE 100000000
#define INITIAL_ARRAY_CAPACITY 16
#define ARRAY_GROWTH_FACTOR 2

#ifndef M_PI
#define M_PI 3.14159265358979323846
#endif

#ifndef EPSILON
#define EPSILON 1e-7f
#endif

//
// Helpers
//

/*
 * Accumulates the gradient into the tensor's .grad field.
 * Handles the first gradient arrival by initializing to zero before adding.
 * Subsequent calls accumulate via addition.
 *
 * tensor_mut: The tensor to update (mutable).
 * grad:       The gradient to accumulate.
 */
static void accumulate_grad(Tensor *tensor_mut, const Tensor *grad) {
    assert(tensor_mut != NULL);

    if (!tensor_mut->requires_grad) {
        return;
    }

    assert(grad != NULL);
    assert(grad->data != NULL || grad->size == 0);

    if (tensor_mut->grad == NULL) {
        // first gradient: initialize to zero then add to ensure we own the memory
        Tensor *zeros = tensor_zeros(tensor_mut->shape, tensor_mut->ndim, false);
        tensor_mut->grad = tensor_add(zeros, grad);
        tensor_free(zeros);
    } else {
        // subsequent gradients: accumulate via addition
        Tensor *new_grad = tensor_add(tensor_mut->grad, grad);
        tensor_free(tensor_mut->grad);
        tensor_mut->grad = new_grad;
    }
}

//
// Topological Sort
//

/*
 * Dynamic array of GradFn pointers.
 * Used for building the topological sort of the graph.
 */
typedef struct {
    GradFn **data;
    uint64_t size;
    uint64_t capacity;
} PtrArray;

static void ptr_array_init(PtrArray *arr) {
    assert(arr != NULL);
    arr->size = 0;
    arr->capacity = INITIAL_ARRAY_CAPACITY;
    arr->data = (GradFn **)malloc((uint64_t)arr->capacity * sizeof(GradFn *));
    assert(arr->data != NULL && "malloc failed");
}

static void ptr_array_free(PtrArray *arr) {
    assert(arr != NULL);
    if (arr->data) {
        free(arr->data);
        arr->data = NULL;
    }
    arr->size = 0;
    arr->capacity = 0;
}

static void ptr_array_append(PtrArray *arr, GradFn *grad_fn) {
    assert(arr != NULL);
    assert(grad_fn != NULL);

    if (arr->size >= arr->capacity) {
        arr->capacity *= ARRAY_GROWTH_FACTOR;
        GradFn **new_data = (GradFn **)realloc(arr->data, (uint64_t)arr->capacity * sizeof(GradFn *));
        assert(new_data != NULL && "realloc failed");
        arr->data = new_data;
    }
    arr->data[arr->size++] = grad_fn;
}

static bool ptr_array_contains(const PtrArray *arr, const GradFn *grad_fn) {
    assert(arr != NULL);
    assert(arr->size < MAX_ARRAY_SIZE && "Array size exceeds maximum limit");
    for (uint64_t idx = 0; idx < arr->size; idx++) {
        if (arr->data[idx] == grad_fn) {
            return true;
        }
    }
    return false;
}

/*
 * Recursive helper for topological sort via DFS.
 * Post-order traversal ensures dependencies come before dependents.
 */
static void build_topo_recursive(GradFn *grad_fn, PtrArray *topo, PtrArray *visited, uint64_t depth) {
    assert(depth < MAX_RECURSION_DEPTH && "Recursion depth exceeded: graph too deep");
    assert(grad_fn != NULL);
    assert(topo != NULL);
    assert(visited != NULL);

    if (ptr_array_contains(visited, grad_fn)) {
        return;
    }
    ptr_array_append(visited, grad_fn);

    // visit children first (post-order traversal)
    for (int64_t child_idx = 0; child_idx < grad_fn->num_next; child_idx++) {
        if (grad_fn->next_fns[child_idx]) {
            build_topo_recursive(grad_fn->next_fns[child_idx], topo, visited, depth + 1);
        }
    }

    // add current node after all children have been visited
    ptr_array_append(topo, grad_fn);
}

//
// Autograd Engine
//

/*
 * Computes gradients of root tensor w.r.t. all graph leaves via backpropagation.
 * Uses reverse-mode automatic differentiation: builds computation graph topology,
 * then propagates gradients from root to leaves in reverse topological order.
 */
void backward(Tensor *root, const Tensor *grad) {
    assert(root != NULL);

    // seed the gradient at root node
    if (root->grad == NULL) {
        if (grad == NULL) {
            // scalar loss with implicit gradient of 1.0
            if (root->size == 1) {
                const uint64_t shape[] = {1};
                root->grad = tensor_create(NULL, shape, 0, false);
                root->grad->data[0] = 1.0f;
            } else {
                assert(false && "Grad must be specified for non-scalar root");
            }
        } else {
            // explicit gradient provided by caller
            assert(grad->data != NULL || grad->size == 0);
            // copy gradient to ensure we own the memory
            root->grad = tensor_create(grad->data, grad->shape, grad->ndim, false);
        }
    } else {
        // gradient already exists, accumulate new gradient
        if (grad != NULL) {
            accumulate_grad(root, grad);
        }
    }

    if (!root->grad_fn) {
        // leaf node has no backward function to propagate through
        return;
    }

    // build topological sort of computation graph
    PtrArray topo;
    ptr_array_init(&topo);
    PtrArray visited;
    ptr_array_init(&visited);

    build_topo_recursive(root->grad_fn, &topo, &visited, 0);

    // propagate gradients in reverse topological order (outputs to inputs)
    for (int64_t topo_idx = (int64_t)topo.size - 1; topo_idx >= 0; topo_idx--) {
        GradFn *grad_fn = topo.data[topo_idx];
        assert(grad_fn != NULL);

        Tensor *output_tensor = grad_fn->out_tensor;
        // propagate gradient only if output has accumulated gradient
        if (output_tensor && output_tensor->grad) {
            grad_fn->apply(grad_fn, output_tensor->grad);
        }
    }

    ptr_array_free(&topo);
    ptr_array_free(&visited);
}

void grad_fn_init(GradFn *fn, void (*apply)(GradFn *, const struct Tensor *), GradFn **next_fns, int64_t num_next, const char *name) {
    assert(fn != NULL);
    assert(apply != NULL);
    // next_fns can be NULL if num_next is 0

    fn->apply = apply;
    fn->next_fns = next_fns;
    fn->num_next = num_next;
    fn->name = name ? strdup(name) : NULL;
    fn->out_tensor = NULL;
}

void grad_fn_free(GradFn *fn) {
    if (!fn) {
        return;
    }
    if (fn->next_fns) {
        free(fn->next_fns);
    }
    if (fn->name) {
        free(fn->name);
    }
    free(fn);
}

//
// Operations Backwards
//

/*
 * Reverses broadcasting that occurred in forward pass.
 * When forward pass broadcast input (e.g., shape (3) -> (2,3)), the backward
 * gradient must be summed along broadcasted dimensions to match input shape.
 *
 * Example:
 *   Forward: A (2,1) + B (3) -> C (2,3)  [both inputs broadcast to (2,3)]
 *   Backward: dL/dC (2,3)
 *     dL/dA = sum(dL/dC, axis=1, keepdim=True) -> (2,1)
 *     dL/dB = sum(dL/dC, axis=0) -> (3)
 */
static Tensor *unbroadcast(const Tensor *grad, const Tensor *input) {
    if (!grad || !input) {
        return NULL;
    }

    const Tensor *curr_grad = grad;
    bool owns_tensor = false;

    // broadcasting adds dimensions on the left, so collapse extra leading dimensions
    // example: grad (2,3,4) with input (3,4) -> sum axis 0 -> (3,4)
    while (curr_grad->ndim > input->ndim) {
        Tensor *summed = tensor_sum(curr_grad, 0, false);
        if (owns_tensor) {
            tensor_free((Tensor *)curr_grad);
        }
        curr_grad = summed;
        owns_tensor = true;
    }

    // now dimensions match; collapse any dims where input had size 1
    // example: grad (2,3) with input (2,1) -> sum axis 1 with keepdim -> (2,1)
    assert(curr_grad->ndim == input->ndim);
    assert(input->ndim < MAX_NDIM && "Number of dimensions exceeds maximum");

    for (uint64_t dim_idx = 0; dim_idx < input->ndim; dim_idx++) {
        // dimension was broadcasted from 1 to N, so sum back to 1
        if (input->shape[dim_idx] == 1 && curr_grad->shape[dim_idx] > 1) {
            Tensor *summed = tensor_sum(curr_grad, (int64_t)dim_idx, true);
            if (owns_tensor) {
                tensor_free((Tensor *)curr_grad);
            }
            curr_grad = summed;
            owns_tensor = true;
        }
    }

    // caller expects to own result, so clone if we never created a new tensor
    if (!owns_tensor) {
        return tensor_create(grad->data, grad->shape, grad->ndim, false);
    }

    return (Tensor *)curr_grad;
}

static void accumulate_grad_unbroadcast(Tensor *t, const Tensor *grad) {
    if (!t || !grad || !t->requires_grad) {
        return;
    }

    Tensor *adj_grad = unbroadcast(grad, t);
    if (adj_grad) {
        accumulate_grad(t, adj_grad);
        tensor_free(adj_grad);
    }
}

//
// Specific Backward Implementations
//

// --- Add ---
typedef struct {
    GradFn base;
    Tensor *a;
    Tensor *b;
} AddBackward;

static void add_apply(GradFn *base, const Tensor *grad_output) {
    assert(base != NULL);
    assert(grad_output != NULL);
    AddBackward *self = (AddBackward *)base;

    accumulate_grad_unbroadcast(self->a, grad_output);
    accumulate_grad_unbroadcast(self->b, grad_output);
}

GradFn *new_add_backward(Tensor *a, Tensor *b) {
    assert(a != NULL);
    assert(b != NULL);

    AddBackward *fn = (AddBackward *)malloc(sizeof(AddBackward));
    assert(fn != NULL && "malloc failed");

    GradFn **next_fns = (GradFn **)malloc(2 * sizeof(GradFn *));
    assert(next_fns != NULL && "malloc failed");

    int64_t next_fn_count = 0;
    if (a->grad_fn)
        next_fns[next_fn_count++] = a->grad_fn;
    if (b->grad_fn)
        next_fns[next_fn_count++] = b->grad_fn;

    grad_fn_init((GradFn *)fn, add_apply, next_fns, next_fn_count, "AddBackward");
    fn->a = a;
    fn->b = b;
    return (GradFn *)fn;
}

// --- Sub ---
typedef struct {
    GradFn base;
    Tensor *a;
    Tensor *b;
} SubBackward;

static void sub_apply(GradFn *base, const Tensor *grad_output) {
    assert(base != NULL);
    assert(grad_output != NULL);
    SubBackward *self = (SubBackward *)base;

    // dA = dOut
    accumulate_grad_unbroadcast(self->a, grad_output);

    // dB = -dOut
    // Create zero tensor to subtract from
    Tensor *zeros = tensor_zeros(grad_output->shape, grad_output->ndim, false);
    Tensor *neg_grad = tensor_sub(zeros, grad_output);
    tensor_free(zeros);

    accumulate_grad_unbroadcast(self->b, neg_grad);
    tensor_free(neg_grad);
}

GradFn *new_sub_backward(Tensor *a, Tensor *b) {
    assert(a != NULL);
    assert(b != NULL);

    SubBackward *fn = (SubBackward *)malloc(sizeof(SubBackward));
    assert(fn != NULL);

    GradFn **next_fns = (GradFn **)malloc(2 * sizeof(GradFn *));
    assert(next_fns != NULL);

    int64_t next_fn_count = 0;
    if (a->grad_fn)
        next_fns[next_fn_count++] = a->grad_fn;
    if (b->grad_fn)
        next_fns[next_fn_count++] = b->grad_fn;

    grad_fn_init((GradFn *)fn, sub_apply, next_fns, next_fn_count, "SubBackward");
    fn->a = a;
    fn->b = b;
    return (GradFn *)fn;
}

// --- Mul ---
typedef struct {
    GradFn base;
    Tensor *a;
    Tensor *b;
} MulBackward;

static void mul_apply(GradFn *base, const Tensor *grad_output) {
    assert(base != NULL);
    assert(grad_output != NULL);
    MulBackward *self = (MulBackward *)base;

    // dA = dOut * B
    Tensor *da = tensor_mul(grad_output, self->b);
    accumulate_grad_unbroadcast(self->a, da);
    tensor_free(da);

    // dB = dOut * A
    Tensor *db = tensor_mul(grad_output, self->a);
    accumulate_grad_unbroadcast(self->b, db);
    tensor_free(db);
}

GradFn *new_mul_backward(Tensor *a, Tensor *b) {
    assert(a != NULL);
    assert(b != NULL);

    MulBackward *fn = (MulBackward *)malloc(sizeof(MulBackward));
    assert(fn != NULL);

    GradFn **next_fns = (GradFn **)malloc(2 * sizeof(GradFn *));
    assert(next_fns != NULL);

    int64_t next_fn_count = 0;
    if (a->grad_fn)
        next_fns[next_fn_count++] = a->grad_fn;
    if (b->grad_fn)
        next_fns[next_fn_count++] = b->grad_fn;

    grad_fn_init((GradFn *)fn, mul_apply, next_fns, next_fn_count, "MulBackward");
    fn->a = a;
    fn->b = b;
    return (GradFn *)fn;
}

// --- Div ---
typedef struct {
    GradFn base;
    Tensor *a;
    Tensor *b;
} DivBackward;

static void div_apply(GradFn *base, const Tensor *grad_output) {
    assert(base != NULL);
    assert(grad_output != NULL);
    DivBackward *self = (DivBackward *)base;

    // dA = dOut / B
    Tensor *da = tensor_div(grad_output, self->b);
    accumulate_grad_unbroadcast(self->a, da);
    tensor_free(da);

    // dB = -dOut * A / (B^2)
    //    = -(dOut * A) / (B * B)

    // -dOut
    Tensor *zeros = tensor_zeros(grad_output->shape, grad_output->ndim, false);
    Tensor *neg_grad = tensor_sub(zeros, grad_output);
    tensor_free(zeros);

    Tensor *num = tensor_mul(neg_grad, self->a);
    tensor_free(neg_grad);

    Tensor *b_sq = tensor_mul(self->b, self->b);
    Tensor *db = tensor_div(num, b_sq);
    tensor_free(num);
    tensor_free(b_sq);

    accumulate_grad_unbroadcast(self->b, db);
    tensor_free(db);
}

GradFn *new_div_backward(Tensor *a, Tensor *b) {
    assert(a != NULL);
    assert(b != NULL);

    DivBackward *fn = (DivBackward *)malloc(sizeof(DivBackward));
    assert(fn != NULL);

    GradFn **next_fns = (GradFn **)malloc(2 * sizeof(GradFn *));
    assert(next_fns != NULL);

    int64_t next_fn_count = 0;
    if (a->grad_fn)
        next_fns[next_fn_count++] = a->grad_fn;
    if (b->grad_fn)
        next_fns[next_fn_count++] = b->grad_fn;

    grad_fn_init((GradFn *)fn, div_apply, next_fns, next_fn_count, "DivBackward");
    fn->a = a;
    fn->b = b;
    return (GradFn *)fn;
}
