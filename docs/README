studying jax forward-mode automatic differentiation internals.

- `jax-inverse-function.ipynb`  -> intro to building interpreters with jax
- `autodiff.py`                 -> barebones autodiff
- `pytorch-ast-optimization.py` -> 2-3x speedup using pytorch

special thanks to ivan yashchuk from nvidia for guidance and feedback!

```
$ python autodiff.py

# Original function:
# 
# def f(x):
#     return exp(x)**3 + cos(x) * x + 10**2
# 
# Transformed function:
# 
# def f_forward_ad(x: DualNum) -> DualNum:
#     return DualNumOps.custom_add(DualNumOps.custom_add(DualNumOps.custom_pow(DualNumOps.custom_exp(x), 3), DualNumOps.custom_mul(DualNumOps.custom_cos(x), x)), (10 ** 2))
# 
# ----------------------------------------------------------------------
# Ran 6 tests in 0.388s
# 
# OK
```
