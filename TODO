TODO:

- extend with small tested modules
    - broadcasting - study code, optimize code quality

study autograd.c/h and tensor.c/h
shape_t can be reused in tensor_t
see if you can refactor some of the ops and maths into ops.c/h

issues:

- "Fake" Batching: The training loop in main.c simulates batching by accumulating gradients over several samples before updating the weights. A true PyTorch-style implementation would collate the batch of inputs into a single, higher-dimensional tensor (e.g., shape [32, 3072]) and perform a single forward and backward pass on the entire batch. The current method is computationally different and much slower, though the gradient accumulation achieves a similar result for SGD.

- Lack of Broadcasting: The tensor operations only work for tensors of identical shapes (e.g., tensor_add). PyTorch supports powerful broadcasting rules that allow operations on tensors with compatible but different shapes. The bias addition in this code only works because the batch size of the forward pass is always 1.
